{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85af3d06-a324-4274-9c74-575a10d5afe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ceb0db-dfe4-4542-958d-3a46c650463e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "print(len(words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c53e1-5da0-4a5f-a7b9-0c7d6e6d4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97b194-448b-4927-a34f-15032e736ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training split, dev/validation split, test split\n",
    "# 80%, 10%, 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03f64c-d64f-482e-af03-ae4202f78147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182580, 3]) torch.Size([182580])\n",
      "torch.Size([22767, 3]) torch.Size([22767])\n",
      "torch.Size([22799, 3]) torch.Size([22799])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672091a2-71b5-4a9a-890a-b2141087edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to compare our gradients to pytorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f\"{s:15s} | exact {str(ex):5s} | approximate {str(app):5s} | maxdiff {maxdiff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3196cedd-869a-4d51-9e58-383b3f43f89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e97ec90-2c6e-42c7-b7d2-caf423b3a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e247cae6-fe6a-4263-b7f8-2b82371f15c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4980, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "          embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ffae9-845b-46f3-8efc-a5f93d2daed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 10]) torch.Size([27, 10]) torch.Size([32, 3])\n",
      "tensor([[ 1,  1,  4],\n",
      "        [18, 14,  1],\n",
      "        [11,  5,  9],\n",
      "        [ 0,  0,  1],\n",
      "        [12, 15, 14]])\n"
     ]
    }
   ],
   "source": [
    "# emb = C[Xb]\n",
    "print(emb.shape, C.shape, Xb.shape)\n",
    "print(Xb[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1850c788-77a1-4b22-8333-2f93d30ed8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 30]), torch.Size([32, 3, 10]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e414496-ce1a-48ee-8d5b-1f40eaadb77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact True  | approximate True  | maxdiff 0.0\n",
      "probs           | exact True  | approximate True  | maxdiff 0.0\n",
      "counts_sum_inv  | exact True  | approximate True  | maxdiff 0.0\n",
      "counts_sum      | exact True  | approximate True  | maxdiff 0.0\n",
      "counts          | exact True  | approximate True  | maxdiff 0.0\n",
      "norm_logits     | exact True  | approximate True  | maxdiff 0.0\n",
      "logit_maxes     | exact True  | approximate True  | maxdiff 0.0\n",
      "logits          | exact True  | approximate True  | maxdiff 0.0\n",
      "h               | exact True  | approximate True  | maxdiff 0.0\n",
      "W2              | exact True  | approximate True  | maxdiff 0.0\n",
      "b2              | exact True  | approximate True  | maxdiff 0.0\n",
      "hpreact         | exact False | approximate True  | maxdiff 9.313225746154785e-10\n",
      "bngain          | exact False | approximate True  | maxdiff 9.313225746154785e-10\n",
      "bnbias          | exact False | approximate True  | maxdiff 3.725290298461914e-09\n",
      "bnraw           | exact False | approximate True  | maxdiff 6.984919309616089e-10\n",
      "bnvar_inv       | exact False | approximate True  | maxdiff 3.725290298461914e-09\n",
      "bnvar           | exact False | approximate True  | maxdiff 4.656612873077393e-10\n",
      "bndiff2         | exact False | approximate True  | maxdiff 1.4551915228366852e-11\n",
      "bndiff          | exact False | approximate True  | maxdiff 4.656612873077393e-10\n",
      "bnmeani         | exact False | approximate True  | maxdiff 1.862645149230957e-09\n",
      "hprebn          | exact False | approximate True  | maxdiff 4.656612873077393e-10\n",
      "embcat          | exact False | approximate True  | maxdiff 1.3969838619232178e-09\n",
      "W1              | exact False | approximate True  | maxdiff 4.6566128730773926e-09\n",
      "b1              | exact False | approximate True  | maxdiff 5.587935447692871e-09\n",
      "emb             | exact False | approximate True  | maxdiff 1.3969838619232178e-09\n",
      "C               | exact False | approximate True  | maxdiff 1.0244548320770264e-08\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = (1.0/probs) * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) # to sqaush the dims to match counts_sum_inv\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum # (addition is pass through and torch.ones_like does shape adjustment)\n",
    "dnorm_logits = (counts) * dcounts # since count = norm_logits.exp()\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True) # sum to convert to logit_maxes.shape - 32, 1\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = -0.5*(bnvar + 1e-5)**-1.5 * dbnvar_inv\n",
    "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += (2*bndiff) * dbndiff2\n",
    "dbnmeani = (-dbndiff).sum(0, keepdim=True)\n",
    "dhprebn = dbndiff.clone()\n",
    "dhprebn += (1.0/n)*(torch.ones_like(hprebn) * dbnmeani)\n",
    "dembcat = dhprebn @ W1.T \n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "  for j in range(Xb.shape[1]):\n",
    "    ix = Xb[k,j]\n",
    "    dC[ix] += demb[k,j]\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789821a-aa6d-4bb2-854f-98983f5f78ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3443045616149902 diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38145b41-eb7a-4394-8ccc-4e946d5e871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact False | approximate True  | maxdiff 6.28642737865448e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da8daa-62ab-4e14-865c-4ec5085e378d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0645, 0.0866, 0.0194, 0.0515, 0.0216, 0.0819, 0.0258, 0.0374, 0.0208,\n",
       "        0.0317, 0.0373, 0.0338, 0.0359, 0.0284, 0.0337, 0.0145, 0.0099, 0.0221,\n",
       "        0.0168, 0.0552, 0.0466, 0.0221, 0.0249, 0.0666, 0.0636, 0.0273, 0.0203],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fffa06d-b224-4f69-a00d-96be52f0956f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0645,  0.0866,  0.0194,  0.0515,  0.0216,  0.0819,  0.0258,  0.0374,\n",
       "        -0.9792,  0.0317,  0.0373,  0.0338,  0.0359,  0.0284,  0.0337,  0.0145,\n",
       "         0.0099,  0.0221,  0.0168,  0.0552,  0.0466,  0.0221,  0.0249,  0.0666,\n",
       "         0.0636,  0.0273,  0.0203], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dlogits*n)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d7a55-7387-4359-9cd5-d0760bc15148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.1642e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ead5df-8fc8-419f-8e7e-5f271870d098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(dlogits.detach(), cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8724e03-8748-4e46-8297-a30cc0626520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(3.5763e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d003c9ec-ebc9-4d07-b204-5628c86fcf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact False | approximate True  | maxdiff 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c3713-0f4c-4aa8-9e17-06bac0f2b437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([64]),\n",
       " torch.Size([32, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bngain.shape, bnvar_inv.shape, dhpreact.shape, bnraw.shape, dhpreact.sum(0).shape, dhprebn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b3cd80-d840-4b91-b618-8fc703a27ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.6819\n",
      "  10000/ 200000: 2.4674\n",
      "  20000/ 200000: 2.3591\n",
      "  30000/ 200000: 2.1078\n",
      "  40000/ 200000: 2.0692\n",
      "  50000/ 200000: 2.4446\n",
      "  60000/ 200000: 2.3112\n",
      "  70000/ 200000: 2.0737\n",
      "  80000/ 200000: 2.0270\n",
      "  90000/ 200000: 2.0261\n",
      " 100000/ 200000: 2.3802\n",
      " 110000/ 200000: 2.0499\n",
      " 120000/ 200000: 2.1160\n",
      " 130000/ 200000: 2.3582\n",
      " 170000/ 200000: 1.9695\n",
      " 180000/ 200000: 2.3072\n",
      " 190000/ 200000: 1.9326\n"
     ]
    }
   ],
   "source": [
    "# PUTTING IT ALL TOGETHER\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "  \n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "lossi = []\n",
    "\n",
    "with torch.no_grad(): # since we can hand calc everything\n",
    "  for i in range(max_steps):\n",
    "    # construct a minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer 1\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    # for t in [logits, h, hpreact, hprebn, embcat, emb]:\n",
    "    #   t.retain_grad()\n",
    "    # loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "    # cmp('logits', dlogits, logits)\n",
    "    dh = dlogits @ W2.T\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    # cmp('hpreact', dhpreact, hpreact)\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    # cmp('bngain', dbngain, bngain)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    # cmp('bnbias', dbnbias, bnbias)\n",
    "    dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    # cmp('hprebn', dhprebn, hprebn)\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    # cmp('W2', dW2, W2)\n",
    "    # cmp('b2', db2, b2)\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    # cmp('W1', dW1, W1)\n",
    "    # cmp('b1', db1, b1)\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    # cmp('embcat', dembcat, embcat)\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    # cmp('emb', demb, emb)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "      for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "    # cmp('C', dC, C)\n",
    "\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    # if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "    #   break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205348a-4e2d-49ff-a3e1-978768bded00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtTUlEQVR4nO3dd3wUZf4H8M+TTaOEmtBLQpciLQcioKjUi4L9QD29U6yH/TyDKCLqiXqHnuXsnv4QERsnCoKinDTpLURawFBCCwihJiHJ8/tjZ5PZzczOzO5sGz7v14sXu7OzM9/M7n7nmaeNkFKCiIicJS7SARARkf2Y3ImIHIjJnYjIgZjciYgciMmdiMiB4iO149TUVJmenh6p3RMRxaQ1a9YcllKmGa0XseSenp6O1atXR2r3REQxSQixy8x6rJYhInIgJnciIgdiciciciAmdyIiB2JyJyJyICZ3IiIHYnInInKgmEvuR06W4Nuc/ZEOg4goqkVsEFOgej+zAACwasJgpKUkRTgaIqLoFHMl90SXO+Q4EeFAiIiiWMwl94eHdgAAJCe4IhwJEVH0irnkLpQSO28OSESkL/aSO1gfQ0RkJOaSOxERGYvZ5C4lK2aIiPTEXHIXrJUhIjIUc8mdiIiMMbkTETlQzCZ31rgTEemL2eRORET6mNyJiBwoZpM7e0ISEemLueQu2BeSiMhQzCV3IiIyFrvJndUyRES6Yi65s1KGiMhYzCV3IiIyFrPJfV7ufjz9zS+RDoOIKCrFXHL3dJZ59IscvLfk18gGQ0QUpWIuuRMRkTEmdyIiB2JyJyJyoJhL7uwKSURkLOaSOxERGWNyj1IjX1uCWz9YFekwiChGxUc6AKvOlYnDNu4tinQIRBTDTJXchRDDhRBbhRB5Qohsjdf/JIQoFEKsV/6NtT/Uc9OyvMOYt+lApMMgohhjWHIXQrgAvA5gCIC9AFYJIWZLKX2Hh86UUo4LQYzntBveXQEAyJ+SFeFIiCiWmCm59wGQJ6XcKaUsBfAJgFGhDUufXbUy47/Mwb0z1tmzsXPUu4t3ou1jcyMdBhFpMJPcmwPYo3q+V1nm6xohxEYhxOdCiJZaGxJC3CGEWC2EWF1YWBhAuPaZsXI3vt6wL6IxxLpn5mxGeUXo516etnwXsl5ZHPL9hMqaXUdxprQ80mFQhJwsKcPYD1fj4PHisO7Xrt4yXwNIl1KeD+B7AB9qrSSlfFtKmSmlzExLSwtoR6vyjwYeZRQ6dKIY32yMzpPMrHV7sfvI6UiHgSf+uwm5+45HOoyAHCgqxjVvLMOjX2yMdCgUIV+tL8CCzQfx8oLtYd2vmeReAEBdEm+hLKskpTwipSxRnr4LoLc94VX326kS3ddGvb4U6dlz/L5/y4HjmPLtFrvDwtnyCkyanYvCE/rxabn5vZUY9/E6nCg+63c9aXDT2OKz5Th0wl0ymPZzPjpM+NbwPUYenLkBl78auyXmaHCyxP255u5j7ycKLzPJfRWA9kKIDCFEIoDRAGarVxBCNFU9HQlgs30hehM+Y1SLz1Zd7m7Yc8zw/de/+TPe/GlHUDH8tK0Q6dlzvC6zfth8CB8sy8eTszfh/SW/YvvBE6a2te/YGQBARYX/9TLG+6/bvuX9lejz7A8AgImzc1FaXmHLTcSPF5fhZEmZ13EmouhnmNyllGUAxgGYD3fS/lRKmSuEmCyEGKmsdp8QIlcIsQHAfQD+FKqAfXmm/V2321x1jR0J76PluwAA61UnE08puaICmPzNLxj52lK/2ygtq8Dot3/G8eKy4AMCsOLX34Lehl4C7/rkfAx68X9Bb5+IwsdUnbuUcq6UsoOUsq2U8lll2UQp5Wzl8XgpZRcpZXcp5SVSSvvrPXS8OH8rAOCqfy8L6P16Ce148dmgqjXOGJR0tx08geU7qxKyjPBNYVfn/4ZOT8zD4u3aDd0Hjhej+Gw5xn+50bDqy4rN+48jPXsOtpm80iGKXeH9jcfc9ANlGvUXe37zbvQrKzeo41AZ+tIir+dfrNmLZTsO4/xJ32GaUkL3+Gp9Af6lahRZmnfY9H7M2H3kNHYUnrR1m2atzHefaJbmHdFdp9MT8zBj5R7d1wH3SWulhauIORv3AwDmc6AWOZRvVXK4xFxyV5d2PQa+sNDrub/eeb4v7VadGHYWnsTDn23ADe+4Bw59/8tBr3Xv/2Q9XlqwrXL5//3snfzd2w/87HzRiwtx2T9/Cvj9lTEoIZSpDsSQqT/hxneXB71tI0NfWoTr3/o5JNv+V5h7GxDFsphL7mat/PU3/LDZnYQPnSjGb6dKMW/Tfpws0a/jNqpK8SfYwVV2tAX4mpdbVRrefuik31J5yVn31U409Or4Zd9xrN19FNe/9TNOl1Z9Xi8t2Bb0tpdsPxzWPueh+FzJXqdLy5D1ymJsKoj8d99OMTdxmFme0mP+lKzKXiT+PPr5Rsxc7V3lsHj7YUgpo2aysjOl5aiR6MKJ4rN4ecF2PDKsI5ITXF7rqNsJyo264Khs2HsMgPtvDtTCLYcCfq/a71UDln7eoX9CsmpH4Unc9N4KXN2rOaZe38P0+3YfOY0/vr8Cn97ZD43rJAe0bzu+Q2XlFSirkNU+cwrOml1HkbvP3UX6o7F9bdmmlBJSAnFxkcsdjiy5B1I14pvYPfYePYP07Dl4cX7Y2oh1nTdxHgqOnUG2cnPwmauqx/zF2gKNd4aHHSXrUDqh9Ez6cm0B5ubsR3r2HMzPPYD9RWdQWlaBJdsPo0KjTm/a8nzsOnIaX60P/NgGO+YAAG58dwU6PTHPcL25OftNj4Z0JyFeXtjtr59tRBufqTnCfZgdmdyt2nvUeBTm6wv994331N3Pz/Wup/9pWyH6T/nRsJ/4zzvNlVD//J+VmJPjboQs00hEq1SNmV9v2K+5jeKz5fhk5e7KH/aaXdZH/QZbtXH0VCmWbD8ccBvFU1/n4i2L4xU8YwoA4J7pawEAd05bg37P/YjOE+fhpvdWYNryXcjZW4T07Dle6wfKzos+M91dD50oxj3T15ruujr1+23IGD8XJWXOHMeQnj0Hr/4QnraaXw+fQt4hd4eIL9buBQDlyj8su6/GsdUyHnmHjLvYDXh+oeE6Rv4+V7tkf89Ha3CqtBx7j55Gu0Ypuu/3JBsj2w6a703z45ZDmPrdVpSWeyfQF+ZtxftLf0XD2kk4fuYsHv5sg+lterz8Q3Cl9JvfX4mcgiKMHZABQD8Jjv8yR3P5f5bmAwDuvLit1/J+z/2AtJQkzB43oNp7/B1jz4ly92+n8Ysy1cGibYUY3aeV378j2nyxxn11Ybb96MNl+QCA4tIKJMW7IKXE6dJy1Epyp4ZTJWXYuLcI/do2DEm84fDP77fh3svah3w/l/zjf9WWvTh/K1o2qBnyfWtxZMm999MLKh8PnrrIz5rGPl+zN6j3n9Io4W47eAJPf+M7Y3KVotP+pyKw4pUf86qNyD180j1FwunSMvx6+FRA2z2hGnylNYBMa7TwvE37MfGrTQBQ2a/dc/GhVyd9yMR0Dhv2HKucwGx/UbHXjU4+X7PX1MjlcPnLx2tx34x1OGuiu+7MVbtxKMyTTb3x0w50eXJ+5TQaD8xcjzHvLA97HIEoPFGC9Ow5mLUusN/s8p1HKn8bdtHqURcujkzu/nrEWPWvIC7p9L4of3xvhd9L7BMl5pL7jJW7kZ49B7uOVCXoT9f474duhnqeGj0/ba0a7HTVv5dV62kw6vWlWOFT1XTXR2tt/7Kv3X0Uo15fitcX5mm+/tfPNmDU6/5HC/sK5YCyORv3Y/aGfYbzGx06XoxHv8jBrR+uwmOzcvCpTpuQ3b5RqvI8dfZbD7hPwoH0JNtZeDKs9fk7lTEiM1YEdqzKKiSufzM03XgjwZHJPVpkPrPA6/nT3/yC73IPGDaseAb2GPHU76kvB63+lrQKzAOe/9Gwh5HvVL+Xv7qk2qRp+4uqThCeWH15eukE6oCyj837q88aefdHa4LadtGZs+j65PzKNon5uQcxN8fcZ+Ph+Tx2FJ7C+8pUGQAMR+R6qomOnCzFxyt242+f+59Vsqy8Ant+Ox32+t3DJ0s0CwLLdx7Bpf/8CZ9oNPoD7gLYY7NyKrsrR4udAV7JRiMmdxsYlXIBYPP+E3hvya+4Y5pxwnnO4qyVVqdUN1r98MlSaxtUnPQzT87gqd6DszxJyJM4p/1sfs72VfnVr3q0ql6+DXLU66r8ozhZUoa1u93bXrPrqGa9/eb9x03dG2CyqirO7gLtlG+3YOALCytPdsGata4A6dlzcPS0/+9C5jMLNAsCOwvdSXKjxsn7ZEkZuj45Hx+v2I3bPlxtOiYpJebnHjAcgR7sldemgqLKfZwuLcMxn2Mw6MWFmL4ictUtZjG5B2lp3mFT/eifUOqaAXP1yKGkvlQOtKAXzA+oQKMXyoHjxabnbPedbgIA9tmU1ABriXf3kdMY8a/FuHfGOtz2wSp8a7Jk7+/4HTxejAdmrvf7/h2FJ/Hu4p2Vz5coU2EYJWMjnpOup6H1hMmJ7axMde07vbXZnjrzcw/gzmlr8NainZqv2zUe5fJXl+AFZc6qIVMXocfk771ezz9yGhNmbdJ6a1Rhcg/Sjco9To0cs7GR1C5CiID76h08Xv3HfMJkW8eBojO2zbfhqRMGYNtYBE/aNTo03/9yEBe9WNXT6octh3C3yV5P/vT9+w+G8/Nc88YyPDNnc2Vi9CRhdciTZucGPFWz1VO3VgkdcJ8on/jvJizapn/nNU/PJyOeE8jeo2cqCyhl5RV45LMNlfXtdslRGuU9BZEJs3KQnj3H8pXRyZIynI7QXbiY3M8xS/MO4xulTn+/Df24/fEkxyVBjHr15fvjUjd4+xuL8MI8c4n/yMmSypK70enHaKoGKaVulVmw1TKnS7wThtbV0AfL8jHNYgO25zOzevvEiV/lam4HcN8m8eb3V1a95nNkPVNfAO4GcqMeKzNW7sbU791dcXMKivDZmr148NOq7rx6x1ZKiRkrdxveGMfjnulVVajTV+wGAFzwXPWr9NKyCr/bfCVM/ex9MbmfY9RXGs99uyWk89VVSIlNBUWYsWq31/Kp328LuFpn5+GTpi42fBPEv/9nbsDTf9dX1Z0He5V/03srMOxl7a64y3YcQXmFDLrr3S6f2yD6Vk2UK5nuQFExsl5ZjBH/WowvNLr3/n3uZkz8ahM2FZi/neFvp6qqgLROLnr8Hder/70MI19dUm25OzFXNc5+vGJ3tXU829X6Zm09cAJrdh3F+C9zcM/0tbj+zZ8NR/HOzTHXZnPz+yvQbdJ3uq97ehqFe4Sq4wcxkX9fBtgn2IwHZ2oPjvI3gVnGeP9zxS/LO4IFJnpYfJcbul4YP20rRKJLv1xUcOwMaiW6/P6dADDl2814Z/GvWD9xCOrVTAwoFt8pq2et054iQV3ifPizDbimdwuv1/Wm3/DQSkx3+XQO+HrDPtSrmYB2jWpXJk6t9/nmdt9kv6+oGCt2HkH2lzmYe99A1Eh0YcHmQ/hFo0eUv+2qDXt5ESZd0RlA1fxJj32Zg5YNauKSTo38bteI1ky1ZuMKJSb3c9ye30JbNWOVUelm1roCU6XEx2Zpj2w1wzNuwHcqCY9blCqGUT2aab7ef8qPpvbj2X7RmbO6yX1/kA3FUsLvzVWCKUz6fg73zlhXbR2zYzYKjp3Bd6pZTJ+Zsxm/Hj6F7YdO4PwW9SrvRRuMQp+rpB+Uie7qB3hiLSkrR1J89E7ixuROMcXK5X+gVpuca+er9cbdH6Od2d4wauv3HEP+4VOmPgt11cbhkyWYm7O/Wv18hZS4+b0V2FGo38fcSpWGp7HVzNQjQOAT3h05WYpm9WoYrldSZn52VjsxuRM5WChG215pccSvx5Lth6sldgB4WeMmLEZxl2tk+0VKdYund4rRfYz9qXDATJlM7kQRslujv74ZVqbX0JrCGHBX1TwyrKPlfefs9d9DyB/P/Y6t8PSs8c21x06f9ZqDacOeY5UD2bYcOIHfPbugWhdEK91vzczAeeGUH3HHRW1MbzOYmwEFgsmdKMZ0fXK+6XX/8Z1+lcObJnsQAe5eSq8tzAtqIj0rVWrq8Qt6Rr+zXHPaCUB7UNWpUvvmnPJ4W2dAlZbZG/bhlTE9bY9BD5M7UYRJ6Z53KD21Fp74b/hGPpoddAYAt35gfpoAO5xVTVO968gpvLO4ehLVS+x6zN7AJJSKzpxFbkERLmyXGvJ9MbkTRYH3VJOKUZUrXqve5z2WdX/K3R8+Z9JQpCQnhHRfHMREFGEvRMEtHM8F0dTtt6w89A22TO5EEWZ2JCQFJ6cg8MbgWMTkTkTkQEzuREQOxORORORATO5ERA7E5E5E5EBM7kREDsTkTkQUZjsP23tbQC1M7kREYXbfjPUh3weTOxGRAzG5ExE5EJM7EZEDMbkTETmQqeQuhBguhNgqhMgTQmT7We8aIYQUQmTaFyIRkbPIMNzGzzC5CyFcAF4HMAJAZwBjhBCdNdZLAXA/gBV2B0lE5CT7ikJ/4xAzJfc+APKklDullKUAPgEwSmO9pwE8DyDytzshIjrHmUnuzQHsUT3fqyyrJIToBaCllHKOvw0JIe4QQqwWQqwuLCy0HCwREZkTdIOqECIOwFQADxutK6V8W0qZKaXMTEtLC3bXRESkw0xyLwDQUvW8hbLMIwVAVwD/E0LkA7gAwGw2qhIRRY6Z5L4KQHshRIYQIhHAaACzPS9KKYuklKlSynQpZTqA5QBGSinDe7t0IiKqZJjcpZRlAMYBmA9gM4BPpZS5QojJQoiRoQ6QiIisizezkpRyLoC5Pssm6qw7KPiwiIgoGByhSkTkQEzuREQOxORORORATO5ERA7E5E5E5EBM7kREDsTkTkTkQEzuREQOxORORORATO5ERA7E5E5E5EAxl9zjRKQjICKKfjGX3ImIyFjMJffrerc0XomI6BwXc8m9VcOakQ6BiCjqxVxyJyIiY0zuREQOxORORORATO5ERA7E5E5E5EBM7kREDsTkTkTkQEzuREQOFHPJfVDHtEiHQEQU9WIuuTeolRjpEIiIol7MJXciIjLG5E5E5EBM7kREDhRzyd3Fu3UQERmKueTeKCU50iEQEUW9mEvuRERkjMmdiMiBmNyJiByIyZ2IyIGY3ImIHIjJnYjIgUwldyHEcCHEViFEnhAiW+P1u4QQOUKI9UKIJUKIzvaHSkREZhkmdyGEC8DrAEYA6AxgjEby/lhK2U1K2QPACwCm2h0oERGZZ6bk3gdAnpRyp5SyFMAnAEapV5BSHlc9rQVA2hciERFZFW9ineYA9qie7wXQ13clIcRfADwEIBHApVobEkLcAeAOAGjVqpXVWImIyCTbGlSllK9LKdsCeBTA4zrrvC2lzJRSZqal8aYbREShYia5FwBoqXreQlmm5xMAVwYRExERBclMcl8FoL0QIkMIkQhgNIDZ6hWEEO1VT7MAbLcvRCIissqwzl1KWSaEGAdgPgAXgPellLlCiMkAVkspZwMYJ4QYDOAsgKMAbgll0ERE5J+ZBlVIKecCmOuzbKLq8f02x0VEREHgCFUiIgdicicicqCYTO439mUfeSIif2IyuXdvWS/SIRARRbWYTO68RTYRkX8xmdyJiMg/JnciIgeKyeR+cUfOS0NE5E9MJvdGKcmRDoGIKKrFZHInIiL/mNyJiByIyZ2IyIGY3ImIHIjJnYjIgZjciYgcyBHJvXaSqWnpiYjOGY5I7pxrhojIW8wm94eHdKh6wuxOROQlZpP7vZdV3ZObuZ2IyFvMJnciItLniOQuBMvuRERqMZ3cr+zRDADQuE5ShCMhIoouMZ3cXx7dE/lTsvDE5Z0rly3866DIBUREFCViOrl7NKiVWPk4I7WW5jrPX9MtXOEQEUWcI0b/dGlWV/e1WokuLH70UpwtrwhjREREkeWI5G5EXbInIjoXOKJaBgB6taoX6RCIiKKGY0run911IaSUkQ6DiCgqOKbk7ooTiHdZ/3Oevaorljx6SQgiIiKKHMckdzPmP3CR1/MB7VJxQ59WaFG/ZoQiIiIKDUcm99nj+msu79gkxev5R2P7eo1uvbpn85DGRUQULo6pc1c7v0U9y+/Z8vRwJLji8OW6AvsDIiIKM0cm90AkJ7giHQIRkW0cWS0DAGP6tAQAXNgu1Wv5okfYeEpEzufY5N61uXvUampt70nFWjU013i6fPxltsdERBQujk3uVQLr+96kbrLNcRARuTWpE/r84tjkLmy4P1NKsrkmiTZp2pOVqZ3fQnv+m9sHZliKCQAyW9dHzUS2Eehp1YBdWym63ae6k1yomEruQojhQoitQog8IUS2xusPCSF+EUJsFEL8IIRobX+o4Vc7qSq5//TIIORPyap8/qcL0ysf10r0fxLIn5KF33drqvlah8Ypmsv9+fDWPlg5YbDl950rEuMdW2Yhh4gLw/2FDH8FQggXgNcBjADQGcAYIURnn9XWAciUUp4P4HMAL9gdaCT460EzaWQXS9uy87OslRTvdeKxKrW290RqzUJcBdWUVVxEXsJx8zgzRZw+APKklDullKUAPgEwSr2ClHKhlPK08nQ5gBb2hhk9Fjx0EX58+GKvZRe2awigeuMtAPRoWS8cYVmyfPxlXhOtNagd2lkz6yQnhHT7RFSdmeTeHMAe1fO9yjI9twH4VusFIcQdQojVQojVhYWF5qOMIu0apaBNWm2vZe0bpSB/Shb6ZNT3Wt6kTjI+ueMCv9sL5v6vX95zoeX3/CGzJeJdcfhobF/8+8Ze7hggsOXp4Xj+mm7o2ryO1/opSfFY8NBFWpsKipl2CgAY0bWJbfu8oE0DAED9mjzZkPPZWjkphLgJQCaAF7Vel1K+LaXMlFJmpqWl2blrS/q3a4ixA4wbMv9xXXdL2/WdlLJezYSgB0eN6dNK97Wufm5Sosdzv9maifFoUb9G5fLkBBf+8Lvq+7p1QAbaNbLeLmCkUUoScp8a5ned9IY1Mfi8xl7L1LdUtOqZK92TxPVu3SDgbWhZ+8QQW7dHZAczyb0AQEvV8xbKMi9CiMEAJgAYKaUssSe80Jg+9gI8rpMkLmzbsPJx79b10bKBOwH6632jN9Vwu0ZVJfxAerc0rJWI567uhjdv6uXVgOsUtQzaDT65o1+1ZddlGtf46X9SAi3q14TW5KFtTV5JaAnmZjB1azjnKmJg+1TjlcIomu/xYEdvPiNmkvsqAO2FEBlCiEQAowHMVq8ghOgJ4C24E/sh+8MMn2m39cW2Z0YEtY1HhnXEx7f3xQvXnl+5rI7yI1aXRC9s29DURzy8a1PLDbh67JrxfvyITgCAr8cNMFy3ueoKwZ+LO3hfzTWpm1yt4SklKR7X9g6uSaetT7UaAKQkJ8BlsQtDj5b18NCQDkHFsuZx/V5Pl3SM3NVtINI02pwocgyTu5SyDMA4APMBbAbwqZQyVwgxWQgxUlntRQC1AXwmhFgvhJits7mw8ZSmGlscLOCKEwF3pfMU4DNSa+HCtqmoqdFFMjkhDtPH9sU39w7Ax7dfENI5bQa0S/XbE6ZNWm0kuuLw4JCqPrcjunp32dRrEujXtiHyp2Shm07/fbWX/tCj8nHjOkno2aoeHs+qfuVkZmCHEMJydZkZQriPlxXPXd2tsr9yoF3b4iy2udx0gX41XaTVqxn4FcywLo2NVyJLTGUxKeVcKWUHKWVbKeWzyrKJUsrZyuPBUsrGUsoeyr+R/rcYesO6NMa/b+yFv1zSLuT7urRTI6/nWj9XdcNp/3apldMjDO/aBEM6a3+x7x7U1uv5OzdnYvrYvobxnN+iLu68uA3+79Y+mPUX7emPAXc//m3PjsClnar2f4/PPj2CqRZSVz3USHBh1j39K/9+X+qxBL60eiPZ6c2beuu+1j2IXk/qqj5f/q6ktBrbk+JDUxi4uV9rzLnP+CpMzd/fZdVrN/QK+L167VJ2XaVeb6IqUEtaiv73tYXJq9lgOHa0hxACv+/WFAkB3J1Jzcyd+xoGkXRccaJaEgfcSW7swDZey4Z0boz+qtKlXqGveb0aGD/iPMTFCctXLnq9dyZknWd6G6/d0BOL/+Y9QZtRQ2jPVvUMT8Rf36t/orJDjUQXknSu2p67qlu1ZVrfjaxuTfHRbVUn4KT4OHx8u/8eU1b4fjqdmlRv7M6ZNNTydieP6oouFhvo1d/F7x7036PqcYPvTzC/U08ngVDIn5KFkd0Du8+Dv2oq3x53oeDY5G43M1fP0k9Z4XylpHr5+dojVaOVp+HHyo+vVlI8WvpMAeB7deNr1j39NSd1Ux/3pnW1SzsPDG6PmQZdTtUGdfQfi5bOzer4ff3tP2biwrYN8doNPTEghA2Lvt9Drc8lxWdcgW+PI19Wq6M81IPh1COtb+nXGs9e1dVr3esyWyJU2qTV1pzew/fk++f+6QFt39/vWktPpSHX/1VZQKFYwuQeAlofXHpqLeRPycLwrvYn93gLFb6xcA/xN27shb4Z5rsrJrji0LeNdhWB1vxAfTIaIH9KlmYVUJbONBFGBndujI9vv8DSuAWjqxmtLcX5fNbxLuP9PX9N9auOYN0zqC2u6+2dsAd2cJ8kruzZvFqpNZTJzCWEV880PZkmusD+76+D0L9dcNVNfdLt7WobKCb3GKaXqP3V9VnVq3U927Zl1ohuTTHzzurdIAOxYeJQZKSa7+Y4aVRVryTfkcjhNsxnAFf7RrVxUXvvHjRmqw17t66v+7rVkikA/G14p2onmks6NsLWZ4ajZ6v6uu1IVpidAM5sY6yZK5T01FqYPtZ6NdprN/SsfHxdZkvUSHAhq5t9A/ACweRuo6dGdsW1vVvgEoMqCLupS0X/uK47Hvu9+fpxf1JrJ2Jge3u64zWrl4zm9WrgySuszsnj/uNG9WgW0H7j4oSlUqN6qgQ760Vv7mdtLr1hXRpX+5vfuKmXVz23lkSd6jOXxkHw15NK7z7ERjwNvkIIpJu8d8IjwzpqLjf7ucWr/ubHs86rPCkP6+KdXOvWTAio4GPmBHr5+c3QWvl72zWqjc1PD0eL+pGdnZTJ3UZN6ibjH9d1D7hHgx3z0Fzbu4Vt3St9B9h8c6+13hQrHrsMKx5z3/QkKd6FpdmXhuTEZ8cl//gR9pwQ9Uwe1bXaMn9hx8fFISneha/HDcDjWefh6p7NTY0U9p0UzuOl0T28nt97aTs8pRGTh1Y33mD4+1uv96mPV48dMD3pnJKA69VMRJu02siZNBR3Xdym2mrBdrl8/ppuaKgzaG3+Axd5jbr2d0UUhip3Jncj/xrdE5d0TAvpzIae0la0j1bs2rwuuul0YdTSuE6y5d46dnjjJu9uda+O6YkrujfzWz3Tx0Idv0eHxsGX7ONE1Zw3Wrq1qIuxA9tgqmqsgBmPZ52HJy7vjJ/HXwrA3YPK48eHL8bDQzuihsVCgNmSuMdbf8w0tZ7v4LGR3QO7SgOqkmZKcoJm+8ekK7pgVRDTZf/hd610B+UlJ7gMR12HE5O7gd6t6+M/f+7jdelnty7N6mDi5Z29BvtY4blsrGPi5iKDLIx6fHWMft/jcDfM+u7PX6LxLeF2aVYXr47paXkEqj+eCdis0GpoE0Lgkzv6oZGN7SQAMHZgG9w2IEOzh5GnuilBaYytkWAuIc29f6ClGDo2SfGamlrr73/+mm6Wpm/w3BvZ4ymLI7fjXXFIS0nSLX1r8Xxv7GzLCgcm9ygghMCtAzIsz1GiLpj887ru+Nqg2iR/ShYyLbTka3X/C0cXLjP7+/6hi7HokUvQo2W9apf10aqnn7lO/ja8k9fz4TbOhqmnT0YDPDKso2ZvGq3jXjMxHhsmDrU0G6mn+2Fygguf3tUPtXzmWNKarK5pvWQ0rpOEJ7I644M/99Hd9rLsS3FLgIPrlmZfavqE2q9NQ9w9qC3m3mft5BbpnmnRcw1BANyDQX47VWr5fdcEOd9KrElwxaFVw5r4r58RuKEc0m72JOfb31/Ptb1b4NreLZCePQcAcIXJqgm9OnYzhBCmR3B7JlarWzMBvVrp97zx9fDQjnh4qHaDqd6FVFK8Cyseq6o6yZ+SVXlcPP5+VTc0U1U1PZZ1HhJccbi8u3ZXVt++/skJLqycMLjadjXjjBN41OfkqzbBQgeGLs3qIN4VF9Rkc2YxuUeZQG67F2nfP3gRhry0KNJheNk8eXhltUMkfXy78XQRau0b1cb2QydNrTvl6m4Y3rUJekz+PpDQTFvw0EW6jbn92zXE9oPm4lUbOyADV/YMbOSnltTaSXheNVGfL8+9C0JBr63MU3JPSY7HmdJylFVIDGyfhuwR+icKOzG5B+Grv/RH8dnySIcRce1DeELq3y4VtRJdGGvxRuI1QnwDcbMl90Yp1hqUv753AErLK/yuM6ZPK8xYuRujVXOqNKubjH1FxZb2ZUZGai2/vXQC6RMOAA8M6RDUrSKt9s3Xmwzw9oEZ+HzNXrx7S6blz6pj4xRsPXjCcL0h5zXGPZe0xeCpi8I6Qp3JPQjBTCZF5qTWTkLu5OGm1s1IrYVfD58KcUShlZzgMuzK+veruuLZK6u6MW59ZjgOFpXgohcXBr3/Tk1ScPB4cci66l3buwU+/HmXbn98Y9YiG9QxDcvyjui+PiGrMyZozFBqhd6JRr20nXK3tnBicifH+Pb+gThrUOq1j/8k887NmVi4NTS3NhDCe2BWUrwLLpuqoOY94J4AbGeh9aoWM568ogv+NrxTwNNqW+WvQTZQlo90hGoHmdzJdkkWSmVLsy/FAZuqE8yUesNlSOfGtgzBd5q4OKHbF3zOfQNi4mbqr47phXeX7MSpknJT1TKRwq6QMcwzpNxoOt7P7+pXeVOJUPHcyPrxrPPQz8I8383r1fA770kkGNWn10x04b7LrN8nwHNcrAwEO5d0aVbXdO+iSGrVsCYmj+paebvGSHd51MOSewyLixOm6vEy0xtY6t8eiNdu6IWz5RVRU3IOxqJHLkH+kep19zmThmLrgRMBH8tBHRth8+TheH7eFuQUFAUbphfPiT7ZpobkVg1qYmD7VDwwOLjbCNrNM8tncohuWmKF0X1Q9e6tHC5M7mQLV5yAKy7yPzg7tGxQU7MEmZKcEPRJMlS9eJrUTcajwzvZ1hsj3hWHabdZ68YZDg8O7oD6NRNt7UYZKp7UHo6bYWthtQxVc6fGhEvnqnDcDs0udw9qGxPVGsGokejC3YPa2jqVhJb+7RriaoMTSBNlvql6Nf23E4R7VLcHS+5UzfgR5+nOkugZWRfs7Qtjxbf3D8TpUo5lONeY6b8/7tJ2aNeodrWphaMFkztZ8tL1PTB3037D2845RUpyQrXb1sWqufcNDOjGHKQtwRXnf5oIzi1DsaR+rUTc2NfajScoOpwrJ+RoE6lJMJjc6ZzWXePGyqHmmYQrlurzKfYwudM57atx1u4uZYebLmiNTk3r4HdRciPlWNCpSQoutnAvgmjgafQN12hcX0zuRGEmhGBit8gzLUIsGdmjGbYdOmF6WmW7MbkTEYVAgisu5Pfm9efc6M9GRHSOYXInInIgJnciIgdiciciciAmdyIiB2JyJyJyICZ3IiIHYnInInIgEam7hQghCgHsCvDtqQAO2xiOXRiXNYzLumiNjXFZE0xcraWUhnMxRCy5B0MIsVpKmRnpOHwxLmsYl3XRGhvjsiYccbFahojIgZjciYgcKFaT+9uRDkAH47KGcVkXrbExLmtCHldM1rkTEZF/sVpyJyIiP5jciYicSEoZU/8ADAewFUAegOwQbL8lgIUAfgGQC+B+ZfkkAAUA1iv/fq96z3glnq0AhhnFCiADwApl+UwAiRbiyweQo8SwWlnWAMD3ALYr/9dXlgsAryj72Qigl2o7tyjrbwdwi2p5b2X7ecp7hUE8HVXHZD2A4wAeiNTxAvA+gEMANqmWhfz46O3DIK4XAWxR9j0LQD1leTqAM6pj92ag+/f3N/qJK+SfHYAk5Xme8nq6ibhmqmLKB7A+AsdLLz9E/DtW7bdgd3IM5T8ALgA7ALQBkAhgA4DONu+jqecDAJACYBuAzsoX/q8a63dW4khSvsg7lDh1YwXwKYDRyuM3AdxtIb58AKk+y16A8oMCkA3geeXx7wF8q3zBLgCwQvUl2an8X1957PkyrlTWFcp7R1j8fA4AaB2p4wXgIgC94J0UQn589PZhENdQAPHK4+dVcaWr1/PZjqX96/2NBnGF/LMDcA+UJAxgNICZRnH5vP5PABMjcLz08kPEv2PV/naryS+S/wD0AzBf9Xw8gPEh3udXAIb4+cJ7xQBgvhKnZqzKB3YYVT9qr/VMxJOP6sl9K4Cmqi/fVuXxWwDG+K4HYAyAt1TL31KWNQWwRbXcaz0TsQ0FsFR5HLHjBZ8feziOj94+/MXl89pVAKb7Wy+Q/ev9jQbHK+Sfnee9yuN4ZT3hLy7VcgFgD4D2kThePvvw5Ieo+I6p/8VanXtzuD9Uj73KspAQQqQD6An3ZSMAjBNCbBRCvC+EqG8Qk97yhgCOSSnLfJabJQF8J4RYI4S4Q1nWWEq5X3l8AEDjAGNrrjz2XW7WaAAzVM+j4XgB4Tk+evsw61a4S2keGUKIdUKIn4QQA1XxWt1/oL+ZUH92le9RXi9S1jdjIICDUsrtqmVhP14++SHqvmOxltzDRghRG8AXAB6QUh4H8AaAtgB6ANgP92VhJAyQUvYCMALAX4QQXreFl+7Tugx3UEKIRAAjAXymLIqW4+UlHMfH6j6EEBMAlAGYrizaD6CVlLIngIcAfCyEqBOq/WuIys9OZQy8CxFhP14a+SGo7VllZh+xltwL4G7Q8GihLLOVECIB7g9uupTySwCQUh6UUpZLKSsAvAOgj0FMesuPAKgnhIgP5G+QUhYo/x+CuxGuD4CDQoimSuxN4W6ICiS2AuWx73IzRgBYK6U8qMQXFcdLEY7jo7cPv4QQfwJwOYAblR8spJQlUsojyuM1cNdndwhw/5Z/M2H67Crfo7xeV1nfL2Xdq+FuXPXEG9bjpZUfAtheyL9jsZbcVwFoL4TIUEqKowHMtnMHQggB4D0Am6WUU1XLm6pWuwrAJuXxbACjhRBJQogMAO3hbhDRjFX5AS8EcK3y/lvgrrczE1stIUSK5zHcddyblBhu0djebAA3C7cLABQpl3XzAQwVQtRXLrmHwl0Xuh/AcSHEBcpxuNlsbPApTUXD8VIJx/HR24cuIcRwAH8DMFJKeVq1PE0I4VIet4H7GO0McP96f6O/uMLx2anjvRbAj56Tm4HBcNdJV1ZdhPN46eWHALYX+u+Yvwr5aPwHd+vzNrjPzhNCsP0BcF/ubISqKxiAaXB3T9qoHOSmqvdMUOLZClXvEr1Y4e5VsBLurk6fAUgyGVsbuHsibIC7G9YEZXlDAD/A3UVqAYAGsqrh6XVl/zkAMlXbulXZfx6AP6uWZ8L9Y94B4DUYdIVU3lML7lJXXdWyiBwvuE8w+wGchbu+8rZwHB+9fRjElQd3vavne+bpPXKN8vmuB7AWwBWB7t/f3+gnrpB/dgCSled5yuttjOJSln8A4C6fdcN5vPTyQ8S/Y77/OP0AEZEDxVq1DBERmcDkTkTkQEzuREQOxORORORATO5ERA7E5E5E5EBM7kREDvT/87bkD49evWoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b868e8-9615-42f3-93fd-4636a741e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab67135-67ca-4fb4-af9e-5e9813d55fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0772347450256348\n",
      "val 2.118682861328125\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "  x, y = {\n",
    "    \"train\": (Xtr, Ytr),\n",
    "    \"val\": (Xdev, Ydev),\n",
    "    \"test\": (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 #+ b1\n",
    "  # hpreact = bngain * ((hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True)) + bnbias\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact)\n",
    "  logits = h @ W2 + b2\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b07abc-7fda-41f8-b517-953f8b966ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.backward()\n",
    "train 2.147221326828003\n",
    "val 2.1646735668182373\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ec1bc-6668-4fa6-9530-6940e180fcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmaizaan.\n",
      "havih.\n",
      "jari.\n",
      "reet.\n",
      "khalaysie.\n",
      "rahnen.\n",
      "amerynci.\n",
      "aqui.\n",
      "ner.\n",
      "kea.\n",
      "chaiiv.\n",
      "kaleigh.\n",
      "ham.\n",
      "pory.\n",
      "quint.\n",
      "shoilea.\n",
      "jadbi.\n",
      "waje.\n",
      "paijaryni.\n",
      "jaxek.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "  out = []\n",
    "  context = [0] * block_size\n",
    "  while True:\n",
    "    emb = C[torch.tensor([context])]\n",
    "    h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "    context = context[1:] + [ix]\n",
    "    out.append(ix)\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ff906-fc16-4da4-98ab-77d605627b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
